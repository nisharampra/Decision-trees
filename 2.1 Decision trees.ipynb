{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "This is an exploratory exercise to allow you to learn more about decision trees and how they might be used in scikit-learn.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "* Go through the notebook and complete the tasks. \n",
    "* Make sure you understand the examples given. If you need help, refer to the documentation links provided or go to the discussion forum. \n",
    "* When a question allows a free-form answer (e.g. what do you observe?), create a new markdown cell below and answer the question in the notebook. \n",
    "* Save your notebooks when you are done.\n",
    "\n",
    "Before you do the tasks below, go through the scikit-learn decision tree tutorial <a href=\"https://scikit-learn.org/stable/modules/tree.html\">here</a>, with the classifier described <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">here</a>. The tutorial contains instructions on how to use decision trees for both classification and regression in Python. \n",
    "\n",
    "**Task 1:**\n",
    "Using what you learnt in the scikit-learn decision tree tutorial, use decision trees for classification on the iris dataset, and for regression on the diabetes dataset (both included in ```sklearn.datasets```). Your code should print the accuracy and the confusion matrix for the classification problem, and mean squared error for the regression. Try comparing the results for different maximum tree depths. \n",
    "\n",
    "Note: You should split your data 80% training and 20% for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2\n",
      "Accuracy: 0.9666666666666667\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Max Depth: 3\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Max Depth: 4\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Max Depth: 5\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Max Depth: None\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the decision tree classifier with different maximum depths\n",
    "for depth in [2, 3, 4, 5, None]:  # Testing different max depths\n",
    "    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Print the accuracy and confusion matrix\n",
    "    print(f\"Max Depth: {depth}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "How would you avoid overfitting in decision trees? Read the decision tree classifier described <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">here</a> for help. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2\n",
      "Mean Squared Error: 3866.038156768628\n",
      "\n",
      "Max Depth: 3\n",
      "Mean Squared Error: 3656.186930948001\n",
      "\n",
      "Max Depth: 4\n",
      "Mean Squared Error: 3594.089844855363\n",
      "\n",
      "Max Depth: 5\n",
      "Mean Squared Error: 3545.4104698436895\n",
      "\n",
      "Max Depth: None\n",
      "Mean Squared Error: 4986.943820224719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the decision tree regressor with different maximum depths\n",
    "for depth in [2, 3, 4, 5, None]:  # Testing different max depths\n",
    "    reg = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = reg.predict(X_test)\n",
    "\n",
    "    # Print the mean squared error\n",
    "    print(f\"Max Depth: {depth}\")\n",
    "    print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Avoiding Overfitting in Decision Trees\n",
    "Overfitting occurs when the decision tree model becomes too complex and captures noise in the training data, reducing its ability to generalize to unseen data. Here are some strategies to avoid overfitting in decision trees:\n",
    "\n",
    "Limit Maximum Depth: Reducing the maximum depth of the tree ensures that the model does not grow too complex.\n",
    "Pruning: After building the tree, you can prune it by removing branches that have little importance in the overall prediction.\n",
    "Min Samples for Split: Increase the minimum number of samples required to split a node (min_samples_split), which prevents the tree from creating too many small nodes.\n",
    "Min Samples per Leaf: Increase the minimum number of samples required to be at a leaf node (min_samples_leaf), which helps prevent overly specific decisions.\n",
    "Use Cross-Validation: Cross-validation ensures that the model is tested on multiple splits of the data, which helps in selecting optimal hyperparameters.\n",
    "Use Random Forests: Instead of using a single decision tree, Random Forests combine the predictions of multiple trees, reducing the likelihood of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
